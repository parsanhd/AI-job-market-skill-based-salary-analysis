{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880bd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'postings.csv'\n",
    "df_or = pd.read_csv(file_path)\n",
    "\n",
    "df = df_or[['job_id','company_name', 'title','description','skills_desc','normalized_salary','formatted_experience_level','formatted_work_type','remote_allowed','posting_domain','location','listed_time','zip_code']]\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({\n",
    "    'company_name': 'Unknown',\n",
    "    'description': '',\n",
    "    'skills_desc': '',\n",
    "    'normalized_salary': 0,\n",
    "    'remote_allowed': False,\n",
    "    'zip_code': 'Unknown'\n",
    "})\n",
    "df.loc[df['remote_allowed']=='1.0','remote_allowed']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ff2175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base dataset (for NLP + analysis)\n",
    "df_base = df.dropna(subset=['job_id','title','description'])\n",
    "df_base = df_base.drop_duplicates(subset='job_id')\n",
    "\n",
    "# Salary dataset (for prediction)\n",
    "df_salary = df_base.copy()\n",
    "df_salary['normalized_salary'] = pd.to_numeric(df_salary['normalized_salary'], errors='coerce')\n",
    "df_salary = df_salary.dropna(subset=['normalized_salary'])\n",
    "df_salary = df_salary[df_salary['normalized_salary'] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0b1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.to_csv('cleaned_postings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279000fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKILL_LIST = [\n",
    "    \n",
    "    # Programming Languages\n",
    "    \"python\", \"java\", \"c\", \"c++\", \"c#\", \"javascript\", \"typescript\",\n",
    "    \"go\", \"golang\", \"rust\", \"scala\", \"kotlin\", \"swift\", \"ruby\", \"php\",\n",
    "    \"r\", \"matlab\", \"bash\", \"shell\", \"powershell\", \"sql\",\n",
    "\n",
    "    # Data Analysis / Scientific Python\n",
    "    \"pandas\", \"numpy\", \"scipy\", \"statsmodels\", \"jupyter\", \"jupyter notebook\",\n",
    "    \"matplotlib\", \"plotly\",\n",
    "\n",
    "    # Databases & Storage\n",
    "    \"postgresql\", \"postgres\", \"mysql\", \"mariadb\", \"sqlite\",\n",
    "    \"mssql\", \"sql server\", \"oracle\",\n",
    "    \"mongodb\", \"cassandra\", \"dynamodb\", \"cosmos db\",\n",
    "    \"redis\", \"elasticsearch\", \"opensearch\", \"neo4j\",\n",
    "    \"snowflake\", \"redshift\", \"bigquery\", \"synapse\", \"teradata\",\n",
    "    \"s3\", \"adls\", \"gcs\",\n",
    "\n",
    "    # File formats / table formats\n",
    "    \"parquet\", \"avro\", \"orc\",\n",
    "    \"delta lake\", \"delta\", \"iceberg\", \"hudi\",\n",
    "\n",
    "    # Big Data / Streaming / Messaging\n",
    "    \"apache spark\", \"spark\", \"pyspark\",\n",
    "    \"hadoop\", \"hdfs\", \"yarn\",\n",
    "    \"hive\", \"presto\", \"trino\",\n",
    "    \"kafka\", \"kinesis\", \"pubsub\", \"google pubsub\", \"pub/sub\",\n",
    "    \"flink\", \"spark streaming\", \"storm\",\n",
    "    \"rabbitmq\", \"sqs\", \"sns\",\n",
    "\n",
    "    # Orchestration / ETL / ELT\n",
    "    \"etl\", \"elt\", \"data pipeline\", \"data pipelines\",\n",
    "    \"airflow\", \"dagster\", \"prefect\", \"luigi\",\n",
    "    \"dbt\", \"fivetran\", \"stitch\",\n",
    "    \"informatica\", \"talend\", \"ssis\",\n",
    "\n",
    "    # Cloud Platforms & Services\n",
    "    \"aws\", \"amazon web services\", \"azure\", \"gcp\", \"google cloud\",\n",
    "    # AWS specifics\n",
    "    \"lambda\", \"api gateway\", \"ecs\", \"ecr\", \"eks\", \"fargate\",\n",
    "    \"sagemaker\", \"cloudwatch\", \"cloudformation\", \"athena\", \"glue\",\n",
    "    # Azure specifics\n",
    "    \"azure functions\", \"aks\", \"acr\", \"azure devops\", \"data factory\",\n",
    "    # GCP specifics\n",
    "    \"cloud functions\", \"cloud run\", \"gke\", \"bigtable\", \"dataflow\",\n",
    "\n",
    "\n",
    "    # Containers / Infra / DevOps\n",
    "    \"docker\", \"kubernetes\", \"helm\",\n",
    "    \"terraform\", \"pulumi\", \"ansible\",\n",
    "    \"jenkins\", \"github actions\", \"gitlab ci\", \"circleci\", \"ci/cd\",\n",
    "    \"nginx\", \"apache\",\n",
    "\n",
    "    # Backend / APIs\n",
    "    \"rest\", \"rest api\", \"graphql\", \"grpc\", \"soap\",\n",
    "    \"fastapi\", \"flask\", \"django\",\n",
    "    \"node.js\", \"nodejs\", \"express\",\n",
    "    \"spring\", \"spring boot\", \".net\", \"asp.net\", \"asp.net core\",\n",
    "\n",
    "    # Frontend (common in full-stack postings)\n",
    "    \"react\", \"next.js\", \"nextjs\", \"vue\", \"angular\",\n",
    "    \"html\", \"css\", \"sass\", \"tailwind\", \"bootstrap\",\n",
    "\n",
    "    # Machine Learning / AI Fundamentals\n",
    "    \"machine learning\", \"deep learning\", \"artificial intelligence\",\n",
    "    \"statistics\", \"time series\", \"forecasting\",\n",
    "    \"natural language processing\", \"nlp\",\n",
    "    \"computer vision\", \"recommendation systems\", \"recommender systems\",\n",
    "    \"a/b testing\", \"ab testing\", \"experiment design\",\n",
    "\n",
    "    # ML models / methods\n",
    "    \"linear regression\", \"logistic regression\",\n",
    "    \"random forest\", \"xgboost\", \"lightgbm\", \"catboost\",\n",
    "    \"svm\", \"naive bayes\", \"k-means\", \"clustering\",\n",
    "    \"neural networks\",\n",
    "\n",
    "    # ML / AI Frameworks\n",
    "    \"scikit-learn\", \"sklearn\",\n",
    "    \"tensorflow\", \"keras\", \"pytorch\",\n",
    "    \"xgboost\", \"lightgbm\",  # (kept here too because postings repeat)\n",
    "    \"hugging face\", \"transformers\",\n",
    "    \"spacy\", \"nltk\",\n",
    "    \"opencv\",\n",
    "    \"mlflow\", \"kubeflow\",\n",
    "\n",
    "    # GenAI / LLM (very common now)\n",
    "    \"llm\", \"large language model\", \"generative ai\", \"genai\",\n",
    "    \"prompt engineering\", \"rag\", \"retrieval augmented generation\",\n",
    "    \"vector database\", \"vector db\", \"embeddings\",\n",
    "    \"langchain\", \"llamaindex\",\n",
    "    \"openai\", \"azure openai\",\n",
    "    \"pinecone\", \"weaviate\", \"milvus\", \"faiss\", \"chromadb\", \"chroma\",\n",
    "\n",
    "    # MLOps / Deployment / Serving\n",
    "    \"mlops\", \"model deployment\", \"model serving\", \"model monitoring\",\n",
    "    \"feature store\",\n",
    "    \"seldon\", \"bentoml\", \"ray serve\",\n",
    "    \"onnx\",\n",
    "\n",
    "    # Observability / Monitoring / Logging\n",
    "    \"prometheus\", \"grafana\",\n",
    "    \"datadog\", \"new relic\",\n",
    "    \"elk\", \"splunk\",\n",
    "\n",
    "    # Security (often in job reqs)\n",
    "    \"oauth\", \"oauth2\", \"openid connect\", \"jwt\",\n",
    "    \"iam\", \"rbac\",\n",
    "    \"encryption\", \"tls\", \"ssl\",\n",
    "\n",
    "    # Testing / Quality\n",
    "    \"unit testing\", \"integration testing\", \"e2e testing\",\n",
    "    \"pytest\", \"unittest\", \"junit\",\n",
    "    \"selenium\", \"cypress\",\n",
    "\n",
    "    # Version Control / Collaboration\n",
    "    \"git\", \"github\", \"gitlab\", \"bitbucket\",\n",
    "    \"agile\", \"scrum\", \"jira\", \"confluence\",\n",
    "\n",
    "    # BI / Analytics Tools\n",
    "    \"excel\", \"power bi\", \"tableau\", \"looker\", \"metabase\", \"superset\",\n",
    "\n",
    "    # Data Modeling / Warehousing Concepts\n",
    "    \"data modeling\", \"dimensional modeling\", \"star schema\", \"snowflake schema\",\n",
    "    \"data warehousing\", \"data lake\", \"lakehouse\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "491f22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(name) for name in SKILL_LIST]\n",
    "matcher.add(\"Skills\", patterns)\n",
    "desc = df_base['description'].fillna(\"\").astype(str)\n",
    "skills = df_base['skills_desc'].fillna(\"\").astype(str)\n",
    "\n",
    "desc_skill = (desc + \" \" + skills).str.strip().tolist()\n",
    "batch = 256\n",
    "result = []\n",
    "for i, doc in enumerate(nlp.pipe(desc_skill,batch_size=batch)):\n",
    "    matches = matcher(doc)\n",
    "    skills_found = set()\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        skills_found.add(span.text.lower())\n",
    "    result.append(\"|\".join(sorted(skills_found)))\n",
    "\n",
    "df_base['extracted_skills'] = result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c58b0b15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "('Could not convert 1.0 with type float: tried to convert to boolean', 'Conversion failed for column remote_allowed with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowInvalid\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# df_base.to_csv('cleaned_postings_with_skills.csv', index=False)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf_base\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcleaned_postings.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msnappy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Documents\\AI job market\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Documents\\AI job market\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:3124\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3044\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3045\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Documents\\AI job market\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:482\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m impl = get_engine(engine)\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io.BytesIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Documents\\AI job market\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:191\u001b[39m, in \u001b[36mPyArrowImpl.write\u001b[39m\u001b[34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    189\u001b[39m     from_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpreserve_index\u001b[39m\u001b[33m\"\u001b[39m] = index\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfrom_pandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df.attrs:\n\u001b[32m    194\u001b[39m     df_metadata = {\u001b[33m\"\u001b[39m\u001b[33mPANDAS_ATTRS\u001b[39m\u001b[33m\"\u001b[39m: json.dumps(df.attrs)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Documents\\AI job market\\.venv\\Lib\\site-packages\\pyarrow\\table.pxi:4795\u001b[39m, in \u001b[36mpyarrow.lib.Table.from_pandas\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Documents\\AI job market\\.venv\\Lib\\site-packages\\pyarrow\\pandas_compat.py:653\u001b[39m, in \u001b[36mdataframe_to_arrays\u001b[39m\u001b[34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[39m\n\u001b[32m    651\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, maybe_fut \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[32m    652\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_fut, futures.Future):\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m             arrays[i] = \u001b[43mmaybe_fut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m types = [x.type \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Documents\\AI job market\\.venv\\Lib\\site-packages\\pyarrow\\pandas_compat.py:628\u001b[39m, in \u001b[36mdataframe_to_arrays.<locals>.convert_column\u001b[39m\u001b[34m(col, field)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (pa.ArrowInvalid,\n\u001b[32m    624\u001b[39m         pa.ArrowNotImplementedError,\n\u001b[32m    625\u001b[39m         pa.ArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    626\u001b[39m     e.args += (\n\u001b[32m    627\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConversion failed for column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,)\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field_nullable \u001b[38;5;129;01mand\u001b[39;00m result.null_count > \u001b[32m0\u001b[39m:\n\u001b[32m    630\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mField \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m was non-nullable but pandas column \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    631\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhad \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.null_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m null values\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Documents\\AI job market\\.venv\\Lib\\site-packages\\pyarrow\\pandas_compat.py:622\u001b[39m, in \u001b[36mdataframe_to_arrays.<locals>.convert_column\u001b[39m\u001b[34m(col, field)\u001b[39m\n\u001b[32m    619\u001b[39m     type_ = field.type\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m     result = \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (pa.ArrowInvalid,\n\u001b[32m    624\u001b[39m         pa.ArrowNotImplementedError,\n\u001b[32m    625\u001b[39m         pa.ArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    626\u001b[39m     e.args += (\n\u001b[32m    627\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConversion failed for column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Documents\\AI job market\\.venv\\Lib\\site-packages\\pyarrow\\array.pxi:365\u001b[39m, in \u001b[36mpyarrow.lib.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Documents\\AI job market\\.venv\\Lib\\site-packages\\pyarrow\\array.pxi:91\u001b[39m, in \u001b[36mpyarrow.lib._ndarray_to_array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Documents\\AI job market\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowInvalid\u001b[39m: ('Could not convert 1.0 with type float: tried to convert to boolean', 'Conversion failed for column remote_allowed with type object')"
     ]
    }
   ],
   "source": [
    "# df_base.to_csv('cleaned_postings_with_skills.csv', index=False)\n",
    "df_base.to_parquet('cleaned_postings.parquet',engine='pyarrow', compression='snappy',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
